{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTW Alignment to Virus Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manage all imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from numba import njit\n",
    "from glob import glob\n",
    "from scipy import stats\n",
    "\n",
    "import random, h5py, re, os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define globals for selecting input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "kmer_model_fn, k = f\"{data_dir}/dna_kmer_model.txt\", 6 # 6-mer model\n",
    "virus, other, dna_type, virus_ds, other_ds = \"lambda\", \"human\", \"DNA\", \"0\", \"0\"\n",
    "ref_fn = f\"{data_dir}/{virus}/{dna_type}/{virus_ds}/reference.fasta\"\n",
    "virus_fast5_dir = f\"{data_dir}/{virus}/{dna_type}/{virus_ds}/fast5\"\n",
    "other_fast5_dir = f\"{data_dir}/{other}/{dna_type}/{other_ds}/fast5\"\n",
    "results_dir = f\"./results/all_modifications\"\n",
    "virus_max_reads = 1000\n",
    "other_max_reads = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal-Based Reference Setup\n",
    "Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasta(fasta_fn):\n",
    "    ''' Get sequence from FASTA filename. '''\n",
    "    with open(fasta_fn, 'r') as fasta:\n",
    "        return ''.join(fasta.read().split('\\n')[1:])\n",
    "\n",
    "def rev_comp(bases):\n",
    "    ''' Get reverse complement of sequence. '''\n",
    "    return bases.replace('A','t').replace('T','a').replace('G','c').replace('C','g').upper()[::-1]\n",
    "\n",
    "def load_model(kmer_model_fn):\n",
    "    ''' Load k-mer model file into Python dict. '''\n",
    "    kmer_model = {}\n",
    "    with open(kmer_model_fn, 'r') as model_file:\n",
    "        for line in model_file:\n",
    "            kmer, current = line.split()\n",
    "            kmer_model[kmer] = float(current)\n",
    "    return kmer_model\n",
    "\n",
    "def discrete_normalize(seq, bits=8, minval=-4, maxval=4):\n",
    "    ''' Approximate normalization which converts signal to integer of desired precision. '''\n",
    "    mean = int(np.mean(seq))\n",
    "    mean_avg_dev = int(np.mean(np.abs(seq - mean)))\n",
    "    norm_seq = (seq - mean) / mean_avg_dev\n",
    "    \n",
    "    norm_seq[norm_seq < minval] = minval\n",
    "    norm_seq[norm_seq > maxval] = maxval\n",
    "    norm_seq = ( (norm_seq - minval) * (2**(bits)/(maxval-minval)) ).astype(int)\n",
    "    return norm_seq\n",
    "\n",
    "def ref_signal(fasta, kmer_model):\n",
    "    ''' Convert reference FASTA to expected reference signal (z-scores). '''\n",
    "    signal = np.zeros(len(fasta))\n",
    "    for kmer_start in range(len(fasta)-k):\n",
    "        signal[kmer_start] = kmer_model[fasta[kmer_start:kmer_start+k]]\n",
    "    return discrete_normalize(signal*100) # increase dist between floats before rounding\n",
    "    #return stats.zscore(signal*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create COVID reference using (z-score normalized) expected k-mer currents for forward/backward reference FASTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_fasta = get_fasta(ref_fn)\n",
    "kmer_model = load_model(kmer_model_fn)\n",
    "fwd_ref_sig = ref_signal(ref_fasta, kmer_model)\n",
    "rev_ref_sig = ref_signal(rev_comp(ref_fasta), kmer_model)\n",
    "ref_sig = np.concatenate((fwd_ref_sig, rev_ref_sig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define preprocessing functions for converting raw FAST5 data to normalized alignable signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stall_end(signal, stall_threshold=3, \n",
    "                  stall_events=2, stall_event_len=3):\n",
    "    ''' Determine the end of the DNA stall region. '''\n",
    "    \n",
    "    # take average of a few samples to reduce variation\n",
    "    events = []\n",
    "    for event in range(0, len(signal), stall_event_len):\n",
    "        events.append(np.mean(signal[event:event+stall_event_len]))\n",
    "    \n",
    "    # find where we exceed threshold for a few consecutive events\n",
    "    above_threshold_count = 0\n",
    "    event_pos = 0\n",
    "    for event in events:\n",
    "        event_pos += 1\n",
    "        if event > stall_threshold:\n",
    "            above_threshold_count += 1\n",
    "        else:\n",
    "            above_threshold_count = 0\n",
    "        if above_threshold_count == stall_events:\n",
    "            break\n",
    "            \n",
    "    # find where we go back below threshold\n",
    "    below_threshold_count = 0\n",
    "    for event in events[event_pos:]:\n",
    "        event_pos += 1\n",
    "        if event < stall_threshold:\n",
    "            below_threshold_count += 1\n",
    "        else:\n",
    "            below_threshold_count = 0\n",
    "        if below_threshold_count == stall_events:\n",
    "            break\n",
    "            \n",
    "    return event_pos * stall_event_len\n",
    "\n",
    "\n",
    "def trim(signal):\n",
    "    ''' Trims signal by detecting stall (and eventually adapter). '''\n",
    "    stall_end = get_stall_end(stats.zscore(signal))\n",
    "    return signal[stall_end+1000 : stall_end+6000], stall_end\n",
    "\n",
    "\n",
    "def filter_outliers(signal, minval=-4, maxval=4):\n",
    "    \n",
    "    # return empty signals as-is\n",
    "    if not len(signal): return signal\n",
    "    \n",
    "    # upper threshold\n",
    "    for idx, x in enumerate(signal):\n",
    "        if x > maxval:\n",
    "            # other values above max -> threshold to max\n",
    "            if (idx+1 < len(signal) and signal[idx+1] > 3) or \\\n",
    "            (idx > 0 and signal[idx-1] > maxval):\n",
    "                signal[idx] = maxval\n",
    "            # otherwise, single outlier -> interpolate\n",
    "            elif idx == 0:\n",
    "                signal[idx] = signal[1]\n",
    "            elif idx+1 == len(signal):\n",
    "                signal[idx] = signal[idx-1]\n",
    "            else:\n",
    "                signal[idx] = (signal[idx-1] + signal[idx+1]) / 2\n",
    "                \n",
    "    # lower threshold\n",
    "    for idx, x in enumerate(signal):\n",
    "        if x < minval:\n",
    "            # other values below min -> threshold to min\n",
    "            if (idx+1 < len(signal) and signal[idx+1] < -3) or \\\n",
    "            (idx > 0 and signal[idx-1] < minval):\n",
    "                signal[idx] = minval\n",
    "            # otherwise, single outlier -> interpolate\n",
    "            elif idx == 0:\n",
    "                signal[idx] = signal[1]\n",
    "            elif idx+1 == len(signal):\n",
    "                signal[idx] = signal[idx-1]\n",
    "            else:\n",
    "                signal[idx] = (signal[idx-1] + signal[idx+1]) / 2\n",
    "                \n",
    "    return signal\n",
    "\n",
    "def preprocess_read(uuid):\n",
    "    ''' Return preprocessed read from specified FAST5 file. '''\n",
    "    readname = f\"read_{uuid}\"\n",
    "    fast5_file = h5py.File(full_index[uuid], 'r')\n",
    "    signal = np.array(fast5_file[readname]['Raw']['Signal'][:], dtype=np.int16)\n",
    "    length = signal.shape[0]\n",
    "    signal, trimmed = trim(signal)\n",
    "    if not len(signal): return np.array([]), trimmed, length\n",
    "    new_signal = np.array(signal, dtype=float)\n",
    "    for start in range(0, len(signal), 500):\n",
    "        new_signal[start:start+500] = \\\n",
    "            discrete_normalize(signal[:start+500])[start:start+500]\n",
    "            #stats.zscore(signal[:start+500])[start:start+500]\n",
    "    #signal = filter_outliers(np.array(new_signal))\n",
    "    #signal = discrete_normalize(signal)\n",
    "    #signal = segment(signal)\n",
    "    return new_signal, trimmed, length\n",
    "\n",
    "def get_index(index_filename):\n",
    "    ''' Read index data structure from file. '''\n",
    "    index_file = open(index_filename, 'r')\n",
    "    index = {}\n",
    "    for line in index_file:\n",
    "        uuid, fname = re.split(r'\\t+', line)\n",
    "        index[uuid] = fname.rstrip()\n",
    "    index_file.close()\n",
    "    return index\n",
    "\n",
    "\n",
    "def create_index(fast5_dir, force=False):\n",
    "    '''\n",
    "    Create file which stores read FAST5 to UUID mappings. \n",
    "    '''\n",
    "\n",
    "    # return existing index if possible\n",
    "    index_fn = f'{fast5_dir}/index.db'\n",
    "    if not force and os.path.exists(index_fn):\n",
    "        return get_index(index_fn)\n",
    "\n",
    "    # remove existing index\n",
    "    if os.path.exists(index_fn):\n",
    "        os.remove(index_fn)\n",
    "\n",
    "    # create new index    \n",
    "    index_file = open(index_fn, 'w')\n",
    "\n",
    "    # iterate through all FAST5 files in directory\n",
    "    for subdir, dirs, files in os.walk(fast5_dir):\n",
    "        for filename in files:\n",
    "            ext = os.path.splitext(filename)[-1].lower()\n",
    "            if ext == \".fast5\":\n",
    "\n",
    "                # print read uuid and filename to index\n",
    "                fast5_file = h5py.File(os.path.join(subdir, filename), 'r')\n",
    "                if 'Raw' in fast5_file: # single-FAST5\n",
    "                    for readname in fast5_file['Raw']['Reads']:\n",
    "                        uuid = fast5_file['Raw']['Reads'][readname].attrs['read_id']\n",
    "                        print('{}\\t{}'.format(uuid.decode('utf-8'), \\\n",
    "                                os.path.join(subdir, filename)), file=index_file)\n",
    "                else: # multi-FAST5\n",
    "                    for readname in fast5_file:\n",
    "                        uuid = readname[5:] # remove 'read_' naming prefix\n",
    "                        print('{}\\t{}'.format(uuid, \\\n",
    "                                os.path.join(subdir, filename)), file=index_file)\n",
    "\n",
    "    # cleanup and return results\n",
    "    index_file.close()\n",
    "    return get_index(index_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(signal):\n",
    "    width = 5\n",
    "    min_obs = 1\n",
    "    npts = int((len(signal)*450)/4000)\n",
    "\n",
    "    # get difference between all neighboring 'width' regions\n",
    "    cumsum = np.cumsum(np.concatenate([[0.0], signal]))\n",
    "    cand_poss = np.argsort(np.abs( (2 * cumsum[width:-width]) -\n",
    "        cumsum[:-2*width] - cumsum[2*width:])).astype(int)[::-1]\n",
    "    vals = np.abs( (2 * cumsum[width:-width]) - cumsum[:-2*width] - cumsum[2*width:])\n",
    "\n",
    "    # keep 'npts' best checkpoints\n",
    "    chkpts = []\n",
    "    cand_idx = 0\n",
    "    ct = 0\n",
    "    blacklist = set()\n",
    "    while ct < npts:\n",
    "        edge_pos = cand_poss[cand_idx]\n",
    "        if edge_pos not in blacklist:\n",
    "            chkpts.append(edge_pos+width)\n",
    "            ct += 1\n",
    "\n",
    "            # blacklist nearby values (only use peaks)\n",
    "            right = 0\n",
    "            while edge_pos+right+1 < len(vals) and vals[edge_pos + right] > vals[edge_pos + right+1]:\n",
    "                right += 1\n",
    "                blacklist.add(edge_pos+right)\n",
    "            left = 0\n",
    "            while edge_pos+left > 0 and vals[edge_pos + left] > vals[edge_pos + left-1]:\n",
    "                left -= 1\n",
    "                blacklist.add(edge_pos+left)\n",
    "        cand_idx += 1\n",
    "\n",
    "    chkpts = np.sort(chkpts)\n",
    "    new_signal = [np.mean(signal[0:chkpts[0]])]\n",
    "    for i in range(len(chkpts)-1):\n",
    "        new_signal.append(np.mean(signal[chkpts[i]:chkpts[i+1]]))\n",
    "    return np.array(new_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create read UUID -> FAST5 filename mapping\n",
    "virus_index = create_index(virus_fast5_dir)\n",
    "other_index = create_index(other_fast5_dir)\n",
    "full_index = {**virus_index, **other_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random subset of reads\n",
    "virus_readnames = random.choices(list(virus_index.keys()), k=virus_max_reads)\n",
    "other_readnames = random.choices(list(other_index.keys()), k=other_max_reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim all reads\n",
    "with mp.Pool() as pool:\n",
    "    virus_reads, virus_trims, virus_lengths = list(map(list, zip(*pool.map(preprocess_read, virus_readnames))))\n",
    "    other_reads, other_trims, other_lengths = list(map(list, zip(*pool.map(preprocess_read, other_readnames))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = ref_sig\n",
    "#aln_lens = np.array([int((x*450)/4000) for x in range(500,5001,500)])\n",
    "aln_lens = np.array(range(500,5001,500))\n",
    "nthresh = len(aln_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit()\n",
    "def sdtw(seq):\n",
    "    ''' Returns minimum alignment score for subsequence DTW. '''\n",
    "    \n",
    "    # initialize cost matrix\n",
    "    cost_mat = np.zeros((len(seq), len(ref)))\n",
    "    cost_mat[0, 0] = abs(seq[0]-ref[0])#*(seq[0]-ref[0])\n",
    "    for i in range(1, len(seq)):\n",
    "        cost_mat[i, 0] = cost_mat[i-1, 0] + abs(seq[i]-ref[0])#*(seq[i]-ref[0])\n",
    "        \n",
    "    # compute entire cost matrix\n",
    "    for i in range(1, len(seq)):\n",
    "        for j in range(1, len(ref)):\n",
    "            cost_mat[i, j] = min(cost_mat[i-1, j-1], cost_mat[i, j-1], cost_mat[i-1, j]) + \\\n",
    "                             abs(seq[i]-ref[j])#*(seq[i]-ref[j]) \n",
    "    \n",
    "    # return cost of optimal alignment\n",
    "    cost_mins = np.zeros((len(aln_lens),))\n",
    "    for i in range(len(aln_lens)):\n",
    "        if aln_lens[i] <= len(seq):\n",
    "            cost_mins[i] = min(cost_mat[aln_lens[i]-1,:])\n",
    "    return cost_mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit cores since each aligner takes ~4GB of RAM to align\n",
    "with mp.Pool(16) as pool:\n",
    "    print(f'Aligning {virus} reads...', flush=True)\n",
    "    virus_scores_list = pool.map(sdtw, virus_reads)\n",
    "    print(f'Aligning {other} reads...', flush=True)\n",
    "    other_scores_list = pool.map(sdtw, other_reads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Errors\n",
    "Look at low-scoring human reads and high-scoring lambda reads to determine reason for error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,5))\n",
    "#plt.plot(virus_reads[100])\n",
    "plt.plot(virus_reads[0])\n",
    "plt.show()\n",
    "#608,  14, 863, 225, 442, 297, 966, 604, 767, 620, 956, 179,  11,\n",
    "#       987, 976, 752, 723, 255, 663, 480, 212, 691, 926, 102, 461, 338,\n",
    "#       113,  63, 483, 973, 950, 176, 394, 681, 661, 481, 569, 634, 740,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vscores = np.zeros((nthresh,len(virus_scores_list)))\n",
    "for idx, scores in enumerate(virus_scores_list):\n",
    "    for i in range(nthresh):\n",
    "        vscores[i,idx] = scores[i]\n",
    "oscores = np.zeros((nthresh,len(other_scores_list)))\n",
    "for idx, scores in enumerate(other_scores_list):\n",
    "    for i in range(nthresh):\n",
    "        oscores[i,idx] = scores[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(vscores[-1])[::-1]\n",
    "#np.argsort(oscores[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vscores[-1,608]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move data to numpy array for easy sorting/calculations\n",
    "virus_scores = np.zeros((nthresh,len(virus_scores_list)))\n",
    "for idx, scores in enumerate(virus_scores_list):\n",
    "    for i in range(nthresh):\n",
    "        virus_scores[i,idx] = scores[i]\n",
    "other_scores = np.zeros((nthresh,len(other_scores_list)))\n",
    "for idx, scores in enumerate(other_scores_list):\n",
    "    for i in range(nthresh):\n",
    "        other_scores[i,idx] = scores[i]\n",
    "        \n",
    "# save results\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "np.save(f\"{results_dir}/prefix_lengths\", aln_lens)\n",
    "np.save(f\"{results_dir}/virus_trims\", virus_trims)\n",
    "np.save(f\"{results_dir}/virus_lengths\", virus_lengths)\n",
    "np.save(f\"{results_dir}/virus_scores\", virus_scores)\n",
    "np.save(f\"{results_dir}/other_trims\", other_trims)\n",
    "np.save(f\"{results_dir}/other_lengths\", other_lengths)\n",
    "np.save(f\"{results_dir}/other_scores\", other_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_lengths = np.load(f\"{results_dir}/prefix_lengths\")\n",
    "virus_trims = np.load(f\"{results_dir}/virus_trims\")\n",
    "virus_lengths = np.load(f\"{results_dir}/virus_lengths\")\n",
    "virus_scores = np.load(f\"{results_dir}/virus_scores\")\n",
    "other_trims = np.load(f\"{results_dir}/other_trims\")\n",
    "other_lengths = np.load(f\"{results_dir}/other_lengths\")\n",
    "other_scores = np.load(f\"{results_dir}/other_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(virus_scores, other_scores, thresh):\n",
    "    ''' Return F-scores (assumes sorted input). '''\n",
    "    fscores = np.zeros(nthresh)\n",
    "    precs = np.zeros(nthresh)\n",
    "    recalls = np.zeros(nthresh)\n",
    "    for i in range(nthresh):\n",
    "        # short reads don't receive a score, so ignore in accuracy metrics\n",
    "        long_virus = np.count_nonzero(virus_scores[i])\n",
    "        short_virus = virus_scores.shape[1]-long_virus\n",
    "        tp = np.searchsorted(virus_scores[i], thresh) - short_virus\n",
    "        fn = long_virus - tp\n",
    "        long_other = np.count_nonzero(other_scores[i])\n",
    "        short_other = other_scores.shape[1]-long_other\n",
    "        fp = np.searchsorted(other_scores[i], thresh) - short_other\n",
    "        precs[i] = 0 if not tp+fp else tp / (tp+fp)\n",
    "        recalls[i] = 0 if not tp+fn else tp / (tp+fn)  \n",
    "        fscores[i] = 0 if not tp+fp+fn else tp / (tp + 0.5*(fp + fn))\n",
    "    return fscores, precs, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort arrays (for fast f-score calculation)\n",
    "virus_scores = np.sort(virus_scores)\n",
    "other_scores = np.sort(other_scores)\n",
    "max_score = max(np.max(virus_scores), np.max(other_scores))\n",
    "\n",
    "# calculate all f-scores, and save the best thresholds\n",
    "best_threshs = np.zeros(nthresh)\n",
    "best_fscores = np.zeros(nthresh)\n",
    "best_precs = np.zeros(nthresh)\n",
    "best_recalls = np.zeros(nthresh)\n",
    "for thresh in np.arange(max_score/100, max_score, max_score/100):\n",
    "    fscores, precs, recalls = get_stats(virus_scores, other_scores, thresh)\n",
    "    for i in range(nthresh):\n",
    "        if fscores[i] > best_fscores[i]:\n",
    "            best_fscores[i] = fscores[i]\n",
    "            best_precs[i] = precs[i]\n",
    "            best_recalls[i] = recalls[i]\n",
    "            best_threshs[i] = thresh + 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot score distribution for each signal prefix length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l in enumerate(aln_lens):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, best_threshs[i]*2)\n",
    "    ax.hist(virus_scores[i][virus_scores[i] > 0], bins=np.arange(1,best_threshs[i]*2, best_threshs[i]/30), facecolor='r', alpha=0.5)\n",
    "    ax.hist(other_scores[i][other_scores[i] > 0], bins=np.arange(1,best_threshs[i]*2, best_threshs[i]/30), facecolor='g', alpha=0.5)\n",
    "    ax.legend([virus, other])\n",
    "    ax.set_xlabel('DTW Alignment Cost')\n",
    "    ax.set_ylabel('Read Count')\n",
    "    ax.axvline(best_threshs[i], color='k', linestyle='--')\n",
    "    ax.set_title('{} Samples'.format(l))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache all results in .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(results_dir, exist_ok=True)\n",
    "np.save(f\"{results_dir}/fscores\", best_fscores)\n",
    "np.save(f\"{results_dir}/precisions\", best_precs)\n",
    "np.save(f\"{results_dir}/recalls\", best_recalls)\n",
    "np.save(f\"{results_dir}/thresholds\", best_threshs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate accuracy plots for alignment method evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dirs = [\"baseline\", \"abs_value\", \"discrete_norm\", \"running_norm\", \"all_modifications\"]\n",
    "fig, axs = plt.subplots(1,3, figsize=(15,5))\n",
    "for d in results_dirs:\n",
    "    fscores = np.load(f\"results/{d}/fscores.npy\")\n",
    "    axs[0].plot(aln_lens, fscores)\n",
    "    precs = np.load(f\"results/{d}/precisions.npy\")\n",
    "    axs[1].plot(aln_lens, precs)\n",
    "    recalls = np.load(f\"results/{d}/recalls.npy\")\n",
    "    axs[2].plot(aln_lens, recalls)\n",
    "axs[0].set_title('F-score')\n",
    "axs[0].set_ylim(0.5,1.03)\n",
    "axs[1].set_ylim(0.5,1.03)\n",
    "axs[2].set_ylim(0.5,1.03)\n",
    "axs[0].set_xlabel('Samples')\n",
    "axs[1].set_xlabel('Samples')\n",
    "axs[2].set_xlabel('Samples')\n",
    "axs[0].set_ylabel('Score')\n",
    "axs[1].set_title('Precision')\n",
    "axs[2].set_title('Recall')\n",
    "axs[2].legend(results_dirs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Until Runtime and Precision\n",
    "Analyze runtime as a function of accuracy and multi-stage thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Run(self, reads, flowcell=Flowcell()):\n",
    "    self.target_coverage = 30.0\n",
    "    self.coverage_bias = 1.0\n",
    "    self.fwd_tr = 400.0           # bases / sec\n",
    "    self.rev_tr = 100_000.0       # bases / sec\n",
    "    self.reads = reads\n",
    "    self.flowcell = flowcell\n",
    "\n",
    "class Reads(self, results_dir):\n",
    "    self.virus_conc = 0.01        # proportion virus\n",
    "    self.threshold_poss = \n",
    "    self.virus_scores = np.load(f'{results_dir}/virus_scores.npy')\n",
    "    self.virus_lengths = np.load(f'{results_dir}/virus_lengths.npy')\n",
    "    self.virus_trims = np.load(f'{results_dir}/virus_trims.npy')\n",
    "    \n",
    "class Flowcell(self, version='minion'):\n",
    "    self.version = version\n",
    "    self.chemistry = 'r9.4.1'\n",
    "    self.sampling_rate = 4000     # samples/sec\n",
    "    self.reversal_latency = 0.070 # sec\n",
    "    self.channels = 512\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(other_lengths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf-venv",
   "language": "python",
   "name": "sf-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
