{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTW Alignment to Virus Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manage all imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from itertools import repeat\n",
    "from numba import njit\n",
    "from glob import glob\n",
    "from scipy import stats\n",
    "from pyguppy_client_lib.pyclient import PyGuppyClient\n",
    "from pyguppy_client_lib.helper_functions import package_read, basecall_with_pyguppy\n",
    "\n",
    "import random, h5py, re, os, mappy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define globals for selecting input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "kmer_model_fn, k = f\"{data_dir}/dna_kmer_model.txt\", 6 # 6-mer model\n",
    "virus, other, dna_type, virus_ds, other_ds = \"lambda\", \"human\", \"DNA\", \"0\", \"2\"\n",
    "ref_fn = f\"{data_dir}/{virus}/{dna_type}/{virus_ds}/reference.fasta\"\n",
    "virus_fast5_dir = f\"{data_dir}/{virus}/{dna_type}/{virus_ds}/fast5\"\n",
    "other_fast5_dir = f\"{data_dir}/{other}/{dna_type}/{other_ds}/fast5\"\n",
    "results_dir = f\"./results/move_bonus_8\"\n",
    "virus_max_reads = 1000\n",
    "other_max_reads = 1000\n",
    "#prefix_lengths = np.array([int((x*450)/4000) for x in range(500,5001,500)])\n",
    "prefix_lengths = np.array(range(500,5001,500))\n",
    "nprefixes = len(prefix_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal-Based Reference Setup\n",
    "Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasta(fasta_fn):\n",
    "    ''' Get sequence from FASTA filename. '''\n",
    "    with open(fasta_fn, 'r') as fasta:\n",
    "        return ''.join(fasta.read().split('\\n')[1:])\n",
    "\n",
    "def rev_comp(bases):\n",
    "    ''' Get reverse complement of sequence. '''\n",
    "    return bases.replace('A','t').replace('T','a').replace('G','c').replace('C','g').upper()[::-1]\n",
    "\n",
    "def load_model(kmer_model_fn):\n",
    "    ''' Load k-mer model file into Python dict. '''\n",
    "    kmer_model = {}\n",
    "    with open(kmer_model_fn, 'r') as model_file:\n",
    "        for line in model_file:\n",
    "            kmer, current = line.split()\n",
    "            kmer_model[kmer] = float(current)\n",
    "    return kmer_model\n",
    "\n",
    "def discrete_normalize(seq, bits=8, minval=-4, maxval=4):\n",
    "    ''' Approximate normalization which converts signal to integer of desired precision. '''\n",
    "    mean = int(np.mean(seq))\n",
    "    mean_avg_dev = int(np.mean(np.abs(seq - mean)))\n",
    "    norm_seq = (seq - mean) / mean_avg_dev\n",
    "    \n",
    "    norm_seq[norm_seq < minval] = minval\n",
    "    norm_seq[norm_seq > maxval] = maxval\n",
    "    norm_seq = ( (norm_seq - minval) * (2**(bits)/(maxval-minval)) ).astype(int)\n",
    "    return norm_seq\n",
    "\n",
    "def ref_signal(fasta, kmer_model):\n",
    "    ''' Convert reference FASTA to expected reference signal (z-scores). '''\n",
    "    signal = np.zeros(len(fasta))\n",
    "    for kmer_start in range(len(fasta)-k):\n",
    "        signal[kmer_start] = kmer_model[fasta[kmer_start:kmer_start+k]]\n",
    "    return discrete_normalize(signal*100) # increase dist between floats before rounding\n",
    "    #return stats.zscore(signal*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create COVID reference using (z-score normalized) expected k-mer currents for forward/backward reference FASTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_fasta = get_fasta(ref_fn)\n",
    "kmer_model = load_model(kmer_model_fn)\n",
    "fwd_ref_sig = ref_signal(ref_fasta, kmer_model)\n",
    "rev_ref_sig = ref_signal(rev_comp(ref_fasta), kmer_model)\n",
    "ref = np.concatenate((fwd_ref_sig, rev_ref_sig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define preprocessing functions for converting raw FAST5 data to normalized alignable signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stall_end(signal, stall_threshold=3, \n",
    "                  stall_events=2, stall_event_len=3):\n",
    "    ''' Determine the end of the DNA stall region. '''\n",
    "    \n",
    "    # take average of a few samples to reduce variation\n",
    "    events = []\n",
    "    for event in range(0, len(signal), stall_event_len):\n",
    "        events.append(np.mean(signal[event:event+stall_event_len]))\n",
    "    \n",
    "    # find where we exceed threshold for a few consecutive events\n",
    "    above_threshold_count = 0\n",
    "    event_pos = 0\n",
    "    for event in events:\n",
    "        event_pos += 1\n",
    "        if event > stall_threshold:\n",
    "            above_threshold_count += 1\n",
    "        else:\n",
    "            above_threshold_count = 0\n",
    "        if above_threshold_count == stall_events:\n",
    "            break\n",
    "            \n",
    "    # find where we go back below threshold\n",
    "    below_threshold_count = 0\n",
    "    for event in events[event_pos:]:\n",
    "        event_pos += 1\n",
    "        if event < stall_threshold:\n",
    "            below_threshold_count += 1\n",
    "        else:\n",
    "            below_threshold_count = 0\n",
    "        if below_threshold_count == stall_events:\n",
    "            break\n",
    "            \n",
    "    return event_pos * stall_event_len\n",
    "\n",
    "\n",
    "def trim(signal):\n",
    "    ''' Trims signal by detecting stall (and eventually adapter). '''\n",
    "    stall_end = get_stall_end(stats.zscore(signal))\n",
    "    return signal[stall_end+1000 : stall_end+6000], stall_end\n",
    "\n",
    "\n",
    "def filter_outliers(signal, minval=-4, maxval=4):\n",
    "    \n",
    "    # return empty signals as-is\n",
    "    if not len(signal): return signal\n",
    "    \n",
    "    # upper threshold\n",
    "    for idx, x in enumerate(signal):\n",
    "        if x > maxval:\n",
    "            # other values above max -> threshold to max\n",
    "            if (idx+1 < len(signal) and signal[idx+1] > 3) or \\\n",
    "            (idx > 0 and signal[idx-1] > maxval):\n",
    "                signal[idx] = maxval\n",
    "            # otherwise, single outlier -> interpolate\n",
    "            elif idx == 0:\n",
    "                signal[idx] = signal[1]\n",
    "            elif idx+1 == len(signal):\n",
    "                signal[idx] = signal[idx-1]\n",
    "            else:\n",
    "                signal[idx] = (signal[idx-1] + signal[idx+1]) / 2\n",
    "                \n",
    "    # lower threshold\n",
    "    for idx, x in enumerate(signal):\n",
    "        if x < minval:\n",
    "            # other values below min -> threshold to min\n",
    "            if (idx+1 < len(signal) and signal[idx+1] < -3) or \\\n",
    "            (idx > 0 and signal[idx-1] < minval):\n",
    "                signal[idx] = minval\n",
    "            # otherwise, single outlier -> interpolate\n",
    "            elif idx == 0:\n",
    "                signal[idx] = signal[1]\n",
    "            elif idx+1 == len(signal):\n",
    "                signal[idx] = signal[idx-1]\n",
    "            else:\n",
    "                signal[idx] = (signal[idx-1] + signal[idx+1]) / 2\n",
    "                \n",
    "    return signal\n",
    "\n",
    "class Read():                                                                                                                                                                                                                                 \n",
    "    def __init__(self, signal, read_id, offset=0, scaling=1.0):                                                                                                                                                                               \n",
    "        self.signal = signal                                                                                                                                                                                                                  \n",
    "        self.read_id = read_id                                                                                                                                                                                                                \n",
    "        self.total_samples = len(signal)                                                                                                                                                                                                      \n",
    "        self.daq_offset = offset                                                                                                                                                                                                              \n",
    "        self.daq_scaling = scaling                                                                                                                                                                                                            \n",
    "        self.read_tag = random.randint(0, int(2**32 - 1))  \n",
    "\n",
    "def ba_preprocess_read(uuid, length):\n",
    "    readname = f\"read_{uuid}\"\n",
    "    fast5_file = h5py.File(full_index[uuid], 'r')\n",
    "    signal = np.array(fast5_file[readname]['Raw']['Signal'][:], dtype=np.int16)\n",
    "    signal, trimmed = trim(signal)\n",
    "    if len(signal) < max(prefix_lengths): return None\n",
    "    signal_dig = fast5_file[readname]['channel_id'].attrs['digitisation']\n",
    "    signal_offset = fast5_file[readname]['channel_id'].attrs['offset']\n",
    "    signal_range = fast5_file[readname]['channel_id'].attrs['range']\n",
    "    signal_scaling = signal_range / signal_dig\n",
    "    return Read(signal, readname, offset=signal_offset, scaling=signal_scaling)\n",
    "\n",
    "    \n",
    "def preprocess_read(uuid):\n",
    "    ''' Return preprocessed read from specified FAST5 file. '''\n",
    "    readname = f\"read_{uuid}\"\n",
    "    fast5_file = h5py.File(full_index[uuid], 'r')\n",
    "    signal = np.array(fast5_file[readname]['Raw']['Signal'][:], dtype=np.int16)\n",
    "    length = signal.shape[0]\n",
    "    signal, trimmed = trim(signal)\n",
    "    if len(signal) < max(prefix_lengths): return None\n",
    "    new_signal = np.array(signal, dtype=float)\n",
    "    for start in range(0, len(signal), 500):\n",
    "        new_signal[start:start+500] = \\\n",
    "            discrete_normalize(signal[:start+500])[start:start+500]\n",
    "            #stats.zscore(signal[:start+500])[start:start+500]\n",
    "    #signal = filter_outliers(np.array(new_signal))\n",
    "    #signal = discrete_normalize(signal)\n",
    "    #signal = segment(signal)\n",
    "    return new_signal, trimmed, length\n",
    "\n",
    "def get_index(index_filename):\n",
    "    ''' Read index data structure from file. '''\n",
    "    index_file = open(index_filename, 'r')\n",
    "    index = {}\n",
    "    for line in index_file:\n",
    "        uuid, fname = re.split(r'\\t+', line)\n",
    "        index[uuid] = fname.rstrip()\n",
    "    index_file.close()\n",
    "    return index\n",
    "\n",
    "\n",
    "def create_index(fast5_dir, force=False):\n",
    "    '''\n",
    "    Create file which stores read FAST5 to UUID mappings. \n",
    "    '''\n",
    "\n",
    "    # return existing index if possible\n",
    "    index_fn = f'{fast5_dir}/index.db'\n",
    "    if not force and os.path.exists(index_fn):\n",
    "        return get_index(index_fn)\n",
    "\n",
    "    # remove existing index\n",
    "    if os.path.exists(index_fn):\n",
    "        os.remove(index_fn)\n",
    "\n",
    "    # create new index    \n",
    "    index_file = open(index_fn, 'w')\n",
    "\n",
    "    # iterate through all FAST5 files in directory\n",
    "    for subdir, dirs, files in os.walk(fast5_dir):\n",
    "        for filename in files:\n",
    "            ext = os.path.splitext(filename)[-1].lower()\n",
    "            if ext == \".fast5\":\n",
    "\n",
    "                # print read uuid and filename to index\n",
    "                fast5_file = h5py.File(os.path.join(subdir, filename), 'r')\n",
    "                if 'Raw' in fast5_file: # single-FAST5\n",
    "                    for readname in fast5_file['Raw']['Reads']:\n",
    "                        uuid = fast5_file['Raw']['Reads'][readname].attrs['read_id']\n",
    "                        print('{}\\t{}'.format(uuid.decode('utf-8'), \\\n",
    "                                os.path.join(subdir, filename)), file=index_file)\n",
    "                else: # multi-FAST5\n",
    "                    for readname in fast5_file:\n",
    "                        uuid = readname[5:] # remove 'read_' naming prefix\n",
    "                        print('{}\\t{}'.format(uuid, \\\n",
    "                                os.path.join(subdir, filename)), file=index_file)\n",
    "\n",
    "    # cleanup and return results\n",
    "    index_file.close()\n",
    "    return get_index(index_fn)\n",
    "\n",
    "def segment(signal):\n",
    "    width = 5\n",
    "    min_obs = 1\n",
    "    npts = int((len(signal)*450)/4000)\n",
    "\n",
    "    # get difference between all neighboring 'width' regions\n",
    "    cumsum = np.cumsum(np.concatenate([[0.0], signal]))\n",
    "    cand_poss = np.argsort(np.abs( (2 * cumsum[width:-width]) -\n",
    "        cumsum[:-2*width] - cumsum[2*width:])).astype(int)[::-1]\n",
    "    vals = np.abs( (2 * cumsum[width:-width]) - cumsum[:-2*width] - cumsum[2*width:])\n",
    "\n",
    "    # keep 'npts' best checkpoints\n",
    "    chkpts = []\n",
    "    cand_idx = 0\n",
    "    ct = 0\n",
    "    blacklist = set()\n",
    "    while ct < npts:\n",
    "        edge_pos = cand_poss[cand_idx]\n",
    "        if edge_pos not in blacklist:\n",
    "            chkpts.append(edge_pos+width)\n",
    "            ct += 1\n",
    "\n",
    "            # blacklist nearby values (only use peaks)\n",
    "            right = 0\n",
    "            while edge_pos+right+1 < len(vals) and vals[edge_pos + right] > vals[edge_pos + right+1]:\n",
    "                right += 1\n",
    "                blacklist.add(edge_pos+right)\n",
    "            left = 0\n",
    "            while edge_pos+left > 0 and vals[edge_pos + left] > vals[edge_pos + left-1]:\n",
    "                left -= 1\n",
    "                blacklist.add(edge_pos+left)\n",
    "        cand_idx += 1\n",
    "\n",
    "    chkpts = np.sort(chkpts)\n",
    "    new_signal = [np.mean(signal[0:chkpts[0]])]\n",
    "    for i in range(len(chkpts)-1):\n",
    "        new_signal.append(np.mean(signal[chkpts[i]:chkpts[i+1]]))\n",
    "    return np.array(new_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create read UUID -> FAST5 filename mapping\n",
    "virus_index = create_index(virus_fast5_dir)\n",
    "other_index = create_index(other_fast5_dir)\n",
    "full_index = {**virus_index, **other_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random subset of reads\n",
    "virus_readnames = random.choices(list(virus_index.keys()), k=virus_max_reads*2)\n",
    "other_readnames = random.choices(list(other_index.keys()), k=other_max_reads*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basecalling and Alignment\n",
    "Initialize aligner and basecaller for DNA lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = mappy.Aligner(\n",
    "    fn_idx_in = ref_fn,\n",
    "    preset = \"map-ont\", # \"splice\" for RNA\n",
    "    best_n = 1,\n",
    "    k = 15 # 14 for RNA\n",
    ")\n",
    "\n",
    "basecaller = PyGuppyClient(\n",
    "    address = \"127.0.0.1:1234\", \n",
    "    config = \"dna_r9.4.1_450bps_fast.cfg\",\n",
    "    server_file_load_timeout=10\n",
    ")\n",
    "basecaller.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basecall(packets):\n",
    "    calls = []                                                                                                                                                                                                                            \n",
    "    sent, rcvd = 0, 0                                                                                                                                                                                                                         \n",
    "    while sent < len(packets):                                                                                                                                                                                                            \n",
    "        success = basecaller.pass_read(packets[sent])                                                                                                                                                                                     \n",
    "        if not success:                                                                                                                                                                                                                       \n",
    "            print('ERROR: Failed to basecall read.')                                                                                                                                                                                          \n",
    "            break                                                                                                                                                                                                                             \n",
    "        else:                                                                                                                                                                                                                                 \n",
    "            sent += 1                                                                                                                                                                                                                         \n",
    "    while rcvd < len(packets):                                                                                                                                                                                                            \n",
    "        result = basecaller.get_completed_reads()                                                                                                                                                                                             \n",
    "        rcvd += len(result)                                                                                                                                                                                                                   \n",
    "        calls.extend(result)\n",
    "    return calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_virus_scores = np.zeros((nprefixes, virus_max_reads))\n",
    "ba_other_scores = np.zeros((nprefixes, other_max_reads))\n",
    "with mp.Pool() as pool:\n",
    "    for prefix_idx, length in enumerate(prefix_lengths):\n",
    "        \n",
    "        # trim reads\n",
    "        ba_virus_reads = list(filter(None, pool.starmap(\n",
    "                    ba_preprocess_read, zip(virus_readnames, repeat(length)))))[:virus_max_reads]\n",
    "        ba_other_reads = list(filter(None, pool.starmap(\n",
    "                    ba_preprocess_read, zip(other_readnames, repeat(length)))))[:other_max_reads]\n",
    "                           \n",
    "        # package read data\n",
    "        virus_pkts = [package_read(\n",
    "            read_tag = read.read_tag, \n",
    "            read_id = read.read_id, \n",
    "            raw_data = read.signal, \n",
    "            daq_offset = float(read.daq_offset), \n",
    "            daq_scaling = float(read.daq_scaling)\n",
    "        ) for read in ba_virus_reads]\n",
    "        other_pkts = [package_read(\n",
    "            read_tag = read.read_tag, \n",
    "            read_id = read.read_id, \n",
    "            raw_data = read.signal, \n",
    "            daq_offset = float(read.daq_offset), \n",
    "            daq_scaling = float(read.daq_scaling)\n",
    "        ) for read in ba_other_reads]\n",
    "\n",
    "        # basecall\n",
    "        virus_calls = basecall(virus_pkts)\n",
    "        other_calls = basecall(other_pkts)\n",
    "        \n",
    "        # align\n",
    "        for call_idx, call in enumerate(virus_calls):\n",
    "            try:\n",
    "                alignment = next(aligner.map(call['datasets']['sequence']))\n",
    "                ba_virus_scores[prefix_idx, call_idx] = alignment.mapq\n",
    "            except(StopIteration):\n",
    "                pass # no alignment\n",
    "        for call_idx, call in enumerate(other_calls):\n",
    "            try:\n",
    "                alignment = next(aligner.map(call['datasets']['sequence']))\n",
    "                ba_other_scores[prefix_idx, call_idx] = alignment.mapq\n",
    "            except(StopIteration):\n",
    "                pass # no alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virus_call_lengths = [len(call['datasets']['sequence']) for call in virus_calls]\n",
    "other_call_lengths = [len(call['datasets']['sequence']) for call in other_calls]\n",
    "plt.hist(virus_call_lengths, bins=np.linspace(0, 800, num=100), facecolor='r', alpha=0.5)\n",
    "plt.hist(other_call_lengths, bins=np.linspace(0, 800, num=100), facecolor='g', alpha=0.5)\n",
    "plt.legend([virus, other])\n",
    "plt.xlabel('Call length (5000 samples)')\n",
    "plt.ylabel('Read Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sDTW Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim all reads\n",
    "with mp.Pool() as pool:\n",
    "    virus_reads, virus_trims, virus_lengths = \\\n",
    "        list(map(list, zip(*filter(None, pool.map(preprocess_read, virus_readnames)))))\n",
    "    other_reads, other_trims, other_lengths = \\\n",
    "        list(map(list, zip(*filter(None, pool.map(preprocess_read, other_readnames)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(virus_reads) < virus_max_reads:\n",
    "    print(f'ERROR: only {len(virus_reads)} virus reads long enough, requested {virus_max_reads}')\n",
    "if len(other_reads) < other_max_reads:\n",
    "    print(f'ERROR: only {len(other_reads)} other reads long enough, requested {other_max_reads}')\n",
    "virus_reads, virus_trims, virus_lengths = virus_reads[:virus_max_reads], \\\n",
    "    virus_trims[:virus_max_reads], virus_lengths[:virus_max_reads]\n",
    "other_reads, other_trims, other_lengths = other_reads[:other_max_reads], \\\n",
    "    other_trims[:other_max_reads], other_lengths[:other_max_reads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit()\n",
    "def sdtw(seq):\n",
    "    ''' Returns minimum alignment score for subsequence DTW. '''\n",
    "    \n",
    "    # initialize cost matrix\n",
    "    cost_mat = np.zeros((len(seq), len(ref)))\n",
    "    cost_mat[0, 0] = abs(seq[0]-ref[0])#*(seq[0]-ref[0])\n",
    "    for i in range(1, len(seq)):\n",
    "        cost_mat[i, 0] = cost_mat[i-1, 0] + abs(seq[i]-ref[0])#*(seq[i]-ref[0])\n",
    "        \n",
    "    # compute entire cost matrix\n",
    "    for i in range(1, len(seq)):\n",
    "        for j in range(1, len(ref)):\n",
    "            #cost_mat[i, j] = min(cost_mat[i-1, j-1], cost_mat[i, j-1], cost_mat[i-1, j]) + \\\n",
    "            #                 abs(seq[i]-ref[j])#*(seq[i]-ref[j]) \n",
    "            cost_mat[i, j] = min(cost_mat[i-1, j-1], cost_mat[i-1, j]) + \\\n",
    "                 abs(seq[i]-ref[j])#*(seq[i]-ref[j]) \n",
    "    \n",
    "    # return cost of optimal alignment\n",
    "    cost_mins = np.zeros((len(prefix_lengths),))\n",
    "    for i in range(len(prefix_lengths)):\n",
    "        if prefix_lengths[i] <= len(seq):\n",
    "            cost_mins[i] = min(cost_mat[prefix_lengths[i]-1,:])\n",
    "    return cost_mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit()\n",
    "def sdtw(seq):\n",
    "    ''' Returns minimum alignment score for subsequence DTW. '''\n",
    "    \n",
    "    # initialize cost matrix\n",
    "    cost_mat = np.zeros((len(seq), len(ref)))\n",
    "    cost_mat[0, 0] = abs(seq[0]-ref[0])\n",
    "    for i in range(1, len(seq)):\n",
    "        cost_mat[i, 0] = cost_mat[i-1, 0] + abs(seq[i]-ref[0])\n",
    "    \n",
    "    prev_consec = np.zeros((len(seq)))\n",
    "    curr_consec = np.zeros((len(seq)))\n",
    "    \n",
    "    # compute entire cost matrix\n",
    "    for j in range(1, len(ref)):\n",
    "        bonus = 32\n",
    "        for i in range(1, len(seq)):\n",
    "            move = cost_mat[i-1, j-1] - prev_consec[i-1]*bonus < cost_mat[i-1, j]\n",
    "            if move:\n",
    "                curr_consec[i] = 0\n",
    "                cost_mat[i, j] = cost_mat[i-1, j-1] - prev_consec[i-1]*bonus + abs(seq[i]-ref[j])\n",
    "            else:\n",
    "                curr_consec[i] = min(10, prev_consec[i] + 1)\n",
    "                cost_mat[i, j] = cost_mat[i-1, j] + abs(seq[i]-ref[j])\n",
    "        prev_consec = curr_consec[:]\n",
    "        curr_consec = np.zeros((len(seq)))\n",
    "    \n",
    "    # return cost of optimal alignment\n",
    "    cost_mins = np.zeros((len(prefix_lengths),))\n",
    "    for i in range(len(prefix_lengths)):\n",
    "        if prefix_lengths[i] <= len(seq):\n",
    "            cost_mins[i] = min(cost_mat[prefix_lengths[i]-1,:])\n",
    "    return cost_mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit cores since each aligner takes ~4GB of RAM to align\n",
    "with mp.Pool(14) as pool:\n",
    "    print(f'Aligning {virus} reads...', flush=True)\n",
    "    virus_scores_list = pool.map(sdtw, virus_reads)\n",
    "    print(f'Aligning {other} reads...', flush=True)\n",
    "    other_scores_list = pool.map(sdtw, other_reads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Errors\n",
    "Look at low-scoring human reads and high-scoring lambda reads to determine reason for error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vscores = np.zeros((nprefixes,len(virus_scores_list)))\n",
    "for idx, scores in enumerate(virus_scores_list):\n",
    "    for i in range(nprefixes):\n",
    "        vscores[i,idx] = scores[i]\n",
    "oscores = np.zeros((nprefixes,len(other_scores_list)))\n",
    "for idx, scores in enumerate(other_scores_list):\n",
    "    for i in range(nprefixes):\n",
    "        oscores[i,idx] = scores[i]\n",
    "high_virus = np.argsort(vscores[-1])[::-1]\n",
    "low_other = np.argsort(oscores[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in high_virus[:20]:\n",
    "    plt.figure(figsize=(25,5))\n",
    "    plt.plot(virus_reads[idx])\n",
    "    plt.title(idx)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move data to numpy array for easy sorting/calculations\n",
    "virus_scores = np.zeros((nprefixes, len(virus_scores_list)))\n",
    "for idx, scores in enumerate(virus_scores_list):\n",
    "    for i in range(nprefixes):\n",
    "        virus_scores[i,idx] = scores[i]\n",
    "other_scores = np.zeros((nprefixes, len(other_scores_list)))\n",
    "for idx, scores in enumerate(other_scores_list):\n",
    "    for i in range(nprefixes):\n",
    "        other_scores[i,idx] = scores[i]\n",
    "        \n",
    "# save results\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "np.save(f\"{results_dir}/prefix_lengths\", prefix_lengths)\n",
    "np.save(f\"{results_dir}/virus_trims\", virus_trims)\n",
    "np.save(f\"{results_dir}/virus_lengths\", virus_lengths)\n",
    "np.save(f\"{results_dir}/virus_scores\", virus_scores)\n",
    "np.save(f\"{results_dir}/other_trims\", other_trims)\n",
    "np.save(f\"{results_dir}/other_lengths\", other_lengths)\n",
    "np.save(f\"{results_dir}/other_scores\", other_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"{results_dir}/ba_virus_scores\", ba_virus_scores)\n",
    "np.save(f\"{results_dir}/ba_other_scores\", ba_other_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_lengths = np.load(f\"{results_dir}/prefix_lengths.npy\")\n",
    "virus_trims = np.load(f\"{results_dir}/virus_trims.npy\")\n",
    "virus_lengths = np.load(f\"{results_dir}/virus_lengths.npy\")\n",
    "virus_scores = np.load(f\"{results_dir}/virus_scores.npy\")\n",
    "other_trims = np.load(f\"{results_dir}/other_trims.npy\")\n",
    "other_lengths = np.load(f\"{results_dir}/other_lengths.npy\")\n",
    "other_scores = np.load(f\"{results_dir}/other_scores.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_virus_scores = np.load(f\"{results_dir}/ba_virus_scores.npy\")\n",
    "ba_other_scores = np.load(f\"{results_dir}/ba_other_scores.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(virus_scores, other_scores, thresh):\n",
    "    ''' Return F-scores (assumes sorted input). '''\n",
    "    fscores = np.zeros(nprefixes)\n",
    "    precs = np.zeros(nprefixes)\n",
    "    recalls = np.zeros(nprefixes)\n",
    "    for i in range(nprefixes):\n",
    "        # short reads don't receive a score, so ignore in accuracy metrics\n",
    "        long_virus = np.count_nonzero(virus_scores[i])\n",
    "        short_virus = virus_scores.shape[1]-long_virus\n",
    "        tp = np.searchsorted(virus_scores[i], thresh) - short_virus\n",
    "        fn = long_virus - tp\n",
    "        long_other = np.count_nonzero(other_scores[i])\n",
    "        short_other = other_scores.shape[1]-long_other\n",
    "        fp = np.searchsorted(other_scores[i], thresh) - short_other\n",
    "        precs[i] = 0 if not tp+fp else tp / (tp+fp)\n",
    "        recalls[i] = 0 if not tp+fn else tp / (tp+fn)  \n",
    "        fscores[i] = 0 if not tp+fp+fn else tp / (tp + 0.5*(fp + fn))\n",
    "    return fscores, precs, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort arrays (for fast f-score calculation)\n",
    "virus_scores = np.sort(virus_scores)\n",
    "other_scores = np.sort(other_scores)\n",
    "min_score = min(np.min(virus_scores), np.min(other_scores))\n",
    "max_score = max(np.max(virus_scores), np.max(other_scores))\n",
    "\n",
    "# calculate all f-scores, and save the best thresholds\n",
    "best_threshs = np.zeros(nprefixes)\n",
    "best_fscores = np.zeros(nprefixes)\n",
    "best_precs = np.zeros(nprefixes)\n",
    "best_recalls = np.zeros(nprefixes)\n",
    "for thresh in np.linspace(min_score, max_score, num=100):\n",
    "    fscores, precs, recalls = get_stats(virus_scores, other_scores, thresh)\n",
    "    for i in range(nprefixes):\n",
    "        if fscores[i] > best_fscores[i]:\n",
    "            best_fscores[i] = fscores[i]\n",
    "            best_precs[i] = precs[i]\n",
    "            best_recalls[i] = recalls[i]\n",
    "            best_threshs[i] = thresh + 0.01\n",
    "            \n",
    "np.save(f\"{results_dir}/fscores\", best_fscores)\n",
    "np.save(f\"{results_dir}/precisions\", best_precs)\n",
    "np.save(f\"{results_dir}/recalls\", best_recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot score distribution for each signal prefix length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, l in enumerate(prefix_lengths):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, best_threshs[i]*2)\n",
    "    minval = min(np.min(virus_scores[i]), np.min(other_scores[i]))\n",
    "    maxval = max(np.max(virus_scores[i]), np.max(other_scores[i]))\n",
    "    ax.hist(virus_scores[i], bins=np.linspace(minval, maxval, num=100), facecolor='r', alpha=0.5)\n",
    "    ax.hist(other_scores[i], bins=np.linspace(minval, maxval, num=100), facecolor='g', alpha=0.5)\n",
    "    ax.legend([virus, other])\n",
    "    ax.set_xlabel('DTW Alignment Cost')\n",
    "    ax.set_ylabel('Read Count')\n",
    "    ax.axvline(best_threshs[i], color='k', linestyle='--')\n",
    "    ax.set_title('{} Samples'.format(l))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basecall Alignment Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l in enumerate([1000]):\n",
    "    fig, ax = plt.subplots()\n",
    "    minval = min(np.min(ba_virus_scores[i]), np.min(ba_other_scores[i]))\n",
    "    maxval = max(np.max(ba_virus_scores[i]), np.max(ba_other_scores[i]))\n",
    "    ax.hist(ba_virus_scores[i], bins=np.linspace(minval, maxval, num=100), facecolor='r', alpha=0.5)\n",
    "    ax.hist(ba_other_scores[i], bins=np.linspace(minval, maxval, num=100), facecolor='g', alpha=0.5)\n",
    "    ax.legend([virus, other])\n",
    "    ax.set_xlabel('Alignment Score')\n",
    "    ax.set_ylabel('Read Count')\n",
    "    ax.set_title('{} Samples'.format(l))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate accuracy plots for alignment method evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_dirs = [\"baseline\", \"abs_value\", \"discrete_norm\", \"running_norm\", \"all_modifications\"]\n",
    "results_dirs = [\"move_baseline\", \"move_bonus_4\", \"move_bonus_8\", \"move_bonus_16\", \"move_bonus_32\"]\n",
    "fig, axs = plt.subplots(1,3, figsize=(15,5))\n",
    "for d in results_dirs:\n",
    "    fscores = np.load(f\"results/{d}/fscores.npy\")\n",
    "    axs[0].plot(prefix_lengths, fscores)\n",
    "    precs = np.load(f\"results/{d}/precisions.npy\")\n",
    "    axs[1].plot(prefix_lengths, precs)\n",
    "    recalls = np.load(f\"results/{d}/recalls.npy\")\n",
    "    axs[2].plot(prefix_lengths, recalls)\n",
    "axs[0].set_title('F-score')\n",
    "axs[0].set_ylim(0.5,1.03)\n",
    "axs[1].set_ylim(0.5,1.03)\n",
    "axs[2].set_ylim(0.5,1.03)\n",
    "axs[0].set_xlabel('Samples')\n",
    "axs[1].set_xlabel('Samples')\n",
    "axs[2].set_xlabel('Samples')\n",
    "axs[0].set_ylabel('Score')\n",
    "axs[1].set_title('Precision')\n",
    "axs[2].set_title('Recall')\n",
    "axs[2].legend(results_dirs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_lengths = [1000, 3000, 5000]\n",
    "ba_lengths = [1000]\n",
    "\n",
    "# initialize plots\n",
    "mpl.rcParams.update({'font.size': 14})\n",
    "dtw_indices = [np.where(prefix_lengths == t)[0] for t in dtw_lengths]\n",
    "ba_indices = [np.where(prefix_lengths == t)[0] for t in ba_lengths]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(0, 1, color='k', marker='*', markersize=15, linestyle='None')                                                                                                                                                                        \n",
    "ax.plot([0,1], [0,1], color='k', marker='None', linestyle=':')\n",
    "\n",
    "# all DTW plots\n",
    "for i, l in zip(dtw_indices, dtw_lengths):\n",
    "    i = int(i)\n",
    "    minval = min(np.min(virus_scores[i]), np.min(other_scores[i]))-1\n",
    "    maxval = max(np.max(virus_scores[i]), np.max(other_scores[i]))+1\n",
    "    thresholds = np.linspace(minval, maxval, num=100)\n",
    "\n",
    "    other_discard_rate, virus_discard_rate = [], []\n",
    "    for t in thresholds:\n",
    "        virus_discard_rate.append(np.sum(virus_scores[i] > t) / len(virus_scores[i]))\n",
    "        other_discard_rate.append(np.sum(other_scores[i] > t) / len(other_scores[i]))\n",
    "    ax.plot(virus_discard_rate, other_discard_rate, marker='o', alpha=0.5)\n",
    "    \n",
    "# all Guppy + Minimap2 plots\n",
    "for i, l in zip(ba_indices, ba_lengths):\n",
    "    i = int(i)\n",
    "    minval = min(np.min(ba_virus_scores[i]), np.min(ba_other_scores[i]))-1\n",
    "    maxval = max(np.max(ba_virus_scores[i]), np.max(ba_other_scores[i]))+1\n",
    "    thresholds = np.linspace(minval, maxval, num=100)\n",
    "\n",
    "    other_discard_rate, virus_discard_rate = [], []\n",
    "    for t in thresholds:\n",
    "        virus_discard_rate.append(sum(ba_virus_scores[i] < t) / len(ba_virus_scores[i]))\n",
    "        other_discard_rate.append(sum(ba_other_scores[i] < t) / len(ba_other_scores[i]))\n",
    "    ax.plot(virus_discard_rate, other_discard_rate, marker='o', alpha=0.5)\n",
    "    \n",
    "ax.set_xlabel('Lambda Phage Discard Rate')\n",
    "ax.set_ylabel('Human Discard Rate')\n",
    "#ax.set_title('Guppy-lite ')\n",
    "ax.legend([\"ideal\", \"random\"] + \\\n",
    "        [f\"SquiggleFilter {x} samples\" for x in dtw_lengths] + \\\n",
    "        [f\"Guppy-lite {x} samples\" for x in ba_lengths], loc=\"lower right\")      \n",
    "ax.set_xlim((-0.1, 1.1))\n",
    "ax.set_ylim((-0.1, 1.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Until Runtime\n",
    "Analyze runtime as a function of accuracy and multi-stage thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reads():\n",
    "    \n",
    "    def __init__(self, results_dir):\n",
    "        self.prop_virus = 0.01        # proportion virus\n",
    "        self.prop_other = 1 - self.prop_virus\n",
    "        self.prefix_lengths = np.load(f'{results_dir}/prefix_lengths.npy')\n",
    "\n",
    "        self.ba_virus_scores = np.load(f'{results_dir}/ba_virus_scores.npy')\n",
    "        self.virus_scores = np.load(f'{results_dir}/virus_scores.npy')\n",
    "        self.virus_lengths = np.load(f'{results_dir}/virus_lengths.npy')\n",
    "        self.avg_virus_length = np.mean(self.virus_lengths)\n",
    "        self.virus_trims = np.load(f'{results_dir}/virus_trims.npy')\n",
    "        self.avg_virus_trim = np.mean(self.virus_trims)\n",
    "\n",
    "        self.ba_other_scores = np.load(f'{results_dir}/ba_other_scores.npy')\n",
    "        self.other_scores = np.load(f\"{results_dir}/other_scores.npy\")\n",
    "        self.other_lengths = np.load(f\"{results_dir}/other_lengths.npy\")\n",
    "        self.avg_other_length = np.mean(self.other_lengths)\n",
    "        self.other_trims = np.load(f\"{results_dir}/other_trims.npy\")\n",
    "        self.avg_other_trim = np.mean(self.other_trims)\n",
    "    \n",
    "    \n",
    "class Flowcell():\n",
    "    \n",
    "    def __init__(self, channels = 512):\n",
    "        self.chemistry = 'r9.4.1'\n",
    "        self.sampling_rate = 4000     # samples/sec\n",
    "        self.minknow_latency = 0.070 # sec\n",
    "        self.channels = channels\n",
    "        \n",
    "        \n",
    "class Classifier():\n",
    "    \n",
    "    def __init__(self, method='sf'):\n",
    "        self.method = method\n",
    "        if method == 'sf': # SquiggleFilter\n",
    "            #self.throughput = 15_318 * 2000\n",
    "            self.throughput = 5000000000000\n",
    "            self.latency = 0.0003264\n",
    "        elif method == 'ba': # BasecallAlign\n",
    "            self.throughput = 550 * 2000\n",
    "            self.latency = 0.149\n",
    "        else:\n",
    "            raise Exception(\"Unknown Read Until classifier type.\")\n",
    "    \n",
    "\n",
    "class Run():\n",
    "    \n",
    "    def __init__(self, reads, clf='sf', flowcell=Flowcell()):\n",
    "        self.flowcell = flowcell\n",
    "        self.clf = Classifier(clf)\n",
    "        self.reads = reads\n",
    "        \n",
    "        self.target_coverage = 30.0\n",
    "        self.target_genome_size = 30_000.0 # bases\n",
    "        self.coverage_bias = 1.0\n",
    "        self.fwd_tr = 400.0           # bases / sec\n",
    "        self.rev_tr = 100_000.0       # bases / sec\n",
    "        self.capture_time = 1.0       # sec\n",
    "\n",
    "        self.sr = self.flowcell.sampling_rate\n",
    "    \n",
    "    def get_simple_runtime(self):\n",
    "        max_throughput = self.flowcell.channels * self.sr\n",
    "        virus_time = self.reads.prop_virus * \\\n",
    "            (self.capture_time + self.reads.avg_virus_length/self.sr)\n",
    "        other_time = self.reads.prop_other * \\\n",
    "            (self.capture_time + self.reads.avg_other_length/self.sr)\n",
    "        useful_time = self.reads.prop_virus * \\\n",
    "            (self.reads.avg_virus_length-self.reads.avg_virus_trim) / self.sr\n",
    "        useful_throughput = max_throughput * useful_time / (virus_time + other_time)\n",
    "        duration = self.target_genome_size * (self.sr/self.fwd_tr) * \\\n",
    "            self.target_coverage * self.coverage_bias / useful_throughput\n",
    "        return duration\n",
    "    \n",
    "    \n",
    "    def get_read_until_runtime(self, prefix_indices, thresholds):\n",
    "        \n",
    "        # get runtime without read until\n",
    "        max_throughput = self.flowcell.channels * self.sr\n",
    "        simple_virus_time = self.reads.prop_virus * \\\n",
    "            (self.capture_time + self.reads.avg_virus_length/self.sr)\n",
    "        simple_other_time = self.reads.prop_other * \\\n",
    "            (self.capture_time + self.reads.avg_other_length/self.sr)\n",
    "        simple_useful_time = self.reads.prop_virus * \\\n",
    "            (self.reads.avg_virus_length-self.reads.avg_virus_trim) / self.sr\n",
    "        simple_prop_useful_time = simple_useful_time / \\\n",
    "            (simple_virus_time + simple_other_time)\n",
    "        \n",
    "        # What percentage of pores can perform Read Until?\n",
    "        # - estimate required basecall throughput from simple sequencing\n",
    "        bc_time = self.reads.prop_virus * self.reads.avg_virus_length/self.sr + \\\n",
    "            self.reads.prop_other * self.reads.avg_other_length/self.sr\n",
    "        bc_throughput = max_throughput * (bc_time/ (simple_virus_time + simple_other_time))\n",
    "        prop_ru = min(1.0, self.clf.throughput/bc_throughput)\n",
    "        prop_simple = 1 - prop_ru\n",
    "        \n",
    "        # calculate sequencing runtime for multiple thresholds\n",
    "        if self.clf.method == 'sf':\n",
    "            rem_virus_scores = self.reads.virus_scores.copy()\n",
    "            rem_other_scores = self.reads.other_scores.copy()\n",
    "        else:\n",
    "            rem_virus_scores = -self.reads.ba_virus_scores.copy()\n",
    "            rem_other_scores = -self.reads.ba_other_scores.copy()   \n",
    "            \n",
    "        eject_virus_time, eject_other_time = 0, 0\n",
    "        for i, thresh in zip(prefix_indices, thresholds):\n",
    "            \n",
    "            # device continues sequencing as we make a read-until decision\n",
    "            length = self.reads.prefix_lengths[i]\n",
    "            samples = length + self.sr * \\\n",
    "                (self.clf.latency + self.flowcell.minknow_latency)\n",
    "            reversal_latency = samples / (self.rev_tr * (self.sr/self.fwd_tr))\n",
    "            latency = self.clf.latency + self.flowcell.minknow_latency + reversal_latency\n",
    "            \n",
    "            # choose which reads to keep, count those ejected\n",
    "            keep_virus = rem_virus_scores[i] < thresh\n",
    "            rem_virus_scores = rem_virus_scores[:,keep_virus]\n",
    "            n_eject_virus = len(keep_virus) - sum(keep_virus)\n",
    "            keep_other = rem_other_scores[i] < thresh\n",
    "            rem_other_scores = rem_other_scores[:,keep_other]\n",
    "            n_eject_other = len(keep_other) - sum(keep_other)\n",
    "            \n",
    "            # update time spent sequencing ejected reads\n",
    "            eject_virus_time += n_eject_virus * self.reads.prop_virus * \\\n",
    "                (self.capture_time + length/self.sr + latency)\n",
    "            eject_other_time += n_eject_other * self.reads.prop_other * \\\n",
    "                (self.capture_time + length/self.sr + latency)\n",
    "            \n",
    "        # update total time spent sequencing each type of read\n",
    "        ru_useful_time = len(rem_virus_scores[0]) * self.reads.prop_virus * \\\n",
    "            (self.reads.avg_virus_length - self.reads.avg_virus_trim) / self.sr\n",
    "        ru_virus_time = eject_virus_time + len(rem_virus_scores[0]) * self.reads.prop_virus * \\\n",
    "            (self.capture_time + self.reads.avg_virus_length / self.sr)\n",
    "        ru_other_time = eject_other_time + len(rem_other_scores[0]) * self.reads.prop_other * \\\n",
    "            (self.capture_time + self.reads.avg_other_length / self.sr)\n",
    "        ru_prop_useful_time = ru_useful_time / (ru_virus_time + ru_other_time)\n",
    "        \n",
    "        # calculate duration based on simple/read until split\n",
    "        prop_useful_time = prop_ru * ru_prop_useful_time + \\\n",
    "            prop_simple * simple_prop_useful_time\n",
    "        useful_throughput = prop_useful_time * max_throughput + 0.0001\n",
    "        duration = self.target_genome_size * (self.sr/self.fwd_tr) * \\\n",
    "            self.target_coverage * self.coverage_bias / useful_throughput\n",
    "        return duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SquiggleFilter Read Until Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find optimal set of thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data = Reads(results_dir)\n",
    "run = Run(read_data)\n",
    "\n",
    "best_threshold_indices = []\n",
    "best_threshold_values = []\n",
    "best_threshold_time = run.get_simple_runtime()\n",
    "for i1, p1 in enumerate(prefix_lengths):\n",
    "    min1 = max(1, min(np.min(virus_scores[i1]), np.min(other_scores[i1])))\n",
    "    max1 = max(np.max(virus_scores[i1]), np.max(other_scores[i1]))\n",
    "    t1s = np.linspace(min1*1.1, max1*0.6, 10)\n",
    "    for i2, p2 in enumerate(prefix_lengths):\n",
    "        times = []\n",
    "        min2 = max(1, min(np.min(virus_scores[i2]), np.min(other_scores[i2])))\n",
    "        max2 = max(np.max(virus_scores[i2]), np.max(other_scores[i2]))\n",
    "        t2s = np.linspace(min2*1.1, max2*0.6, 10)\n",
    "        for t1 in t1s:\n",
    "            for t2 in t2s:\n",
    "                time = run.get_read_until_runtime([i1, i2], [t1, t2])\n",
    "                times.append(time)\n",
    "                if time < best_threshold_time:\n",
    "                    best_threshold_time = time\n",
    "                    best_threshold_indices = [i1,i2]\n",
    "                    best_threshold_values = [t1, t2]\n",
    "        print(f\"\\r{p1}-{p2}: {min(times)}                \", end='')\n",
    "print(f'\\nBest Time: {best_threshold_time}')\n",
    "for i, t in zip(best_threshold_indices, best_threshold_values):\n",
    "    print(f'@ sample {prefix_lengths[i]}: cutoff {t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data = Reads(results_dir)\n",
    "run = Run(read_data)\n",
    "\n",
    "plt.axhline(run.get_simple_runtime(), linestyle=':', color='red')\n",
    "for i1, p1 in enumerate(prefix_lengths[1::2]):\n",
    "    max_score = max(np.max(virus_scores[i1]), np.max(other_scores[i1]))\n",
    "    min_score = max(1, min(np.min(virus_scores[i1]), np.min(other_scores[i1])))\n",
    "    times = []\n",
    "    thresholds = np.linspace(min_score, max_score, 100)\n",
    "    for t in thresholds:\n",
    "        times.append(run.get_read_until_runtime([i1], [t]))\n",
    "    plt.plot(thresholds, times)\n",
    "    print(f\"{p1}: {min(times)}\")\n",
    "plt.axhline(best_threshold_time, linestyle=':', color='black')\n",
    "#plt.ylim(0,600)\n",
    "#plt.xlim(0,20000)\n",
    "plt.ylim(0, run.get_simple_runtime()*1.3)\n",
    "plt.legend(['no Read Until'] + list(prefix_lengths[1::2]) + ['two thresholds'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basecall Align Read Until Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data = Reads(results_dir)\n",
    "run = Run(read_data, 'ba')\n",
    "\n",
    "plt.axhline(run.get_simple_runtime(), linestyle=':', color='red')\n",
    "for i1, p1 in enumerate(prefix_lengths[:3]):\n",
    "    max_score = max(np.max(-ba_virus_scores[i1]), np.max(-ba_other_scores[i1]))\n",
    "    min_score = min(np.min(-ba_virus_scores[i1]), np.min(-ba_other_scores[i1]))\n",
    "    times = []\n",
    "    thresholds = np.linspace(min_score, max_score, 100)\n",
    "    for t in thresholds:\n",
    "        times.append(run.get_read_until_runtime([i1], [t]))\n",
    "    plt.plot(thresholds, times)\n",
    "    print(f\"{p1}: {min(times)}\")\n",
    "#plt.axhline(132.3, linestyle=':', color='green')\n",
    "#plt.ylim(0, run.get_simple_runtime()*1.3)\n",
    "plt.ylim(0,600)\n",
    "#plt.xlim(0, 100_000)\n",
    "plt.legend(['no Read Until'] + list(prefix_lengths[:3]) + ['two thresholds'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot decrease in Read Until sequencing time as channels increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data = Reads(results_dir)\n",
    "\n",
    "simple_runtimes = []\n",
    "sf_runtimes = []\n",
    "ba_runtimes = []\n",
    "channels = np.linspace(512, 51200, num=100)\n",
    "for channel_count in channels:\n",
    "    flowcell = Flowcell(channel_count)\n",
    "    sf_run = Run(read_data, 'sf', flowcell)\n",
    "    ba_run = Run(read_data, 'ba', flowcell)\n",
    "    simple_runtimes.append(sf_run.get_simple_runtime())\n",
    "    sf_runtimes.append(sf_run.get_read_until_runtime(best_threshold_indices, best_threshold_values))\n",
    "    ba_runtimes.append(ba_run.get_read_until_runtime([1], [-10]))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.set_yscale('log')\n",
    "ax.plot(channels, simple_runtimes)\n",
    "ax.plot(channels, sf_runtimes)\n",
    "ax.plot(channels, ba_runtimes)\n",
    "ax.set_xlabel('Portable Sequencer Channels')\n",
    "ax.set_ylabel('Sequencing Time')\n",
    "ax.legend(['No Read Until', 'SquiggleFilter', 'Guppy-lite'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf-venv",
   "language": "python",
   "name": "sf-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
